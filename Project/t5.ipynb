{"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Transformers installation\n! pip install transformers datasets\n! pip install transformers datasets evaluate rouge_score\n\n# To install from source instead of the last release, comment the command above and uncomment the following one.\n# ! pip install git+https://github.com/huggingface/transformers.git","metadata":{"id":"nwmlULlzZlQl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nimport huggingface_hub\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf\")\nsecret_value_1 = user_secrets.get_secret(\"wandb-key\")\n\nwandb.login(key=secret_value_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"huggingface_hub.login(token = secret_value_0 ,write_permission =True )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob","metadata":{"id":"CriB55rKZlQo","execution":{"iopub.status.busy":"2023-10-30T12:03:55.812268Z","iopub.execute_input":"2023-10-30T12:03:55.812636Z","iopub.status.idle":"2023-10-30T12:03:56.160937Z","shell.execute_reply.started":"2023-10-30T12:03:55.812590Z","shell.execute_reply":"2023-10-30T12:03:56.159997Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\nimport glob\n\n\n\n\n# Create a Hugging Face dataset# Use your fine tuning file\nfilename = \"/kaggle/input/bert-cls-in-abs/IN-abs_CLS.xlsx\"\n\ndf = pd.read_excel(filename,index_col=0)\ndf = df.reset_index(drop=True)  # Reset the index without creating a new column\ndf.rename(columns = {'data':'text', 'summary':'summary'}, inplace = True)\nlen(df)\ndataset = Dataset.from_pandas(df[['text', 'summary']])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:03:56.162224Z","iopub.execute_input":"2023-10-30T12:03:56.162708Z","iopub.status.idle":"2023-10-30T12:03:59.665014Z","shell.execute_reply.started":"2023-10-30T12:03:56.162673Z","shell.execute_reply":"2023-10-30T12:03:59.663881Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:03:59.666414Z","iopub.execute_input":"2023-10-30T12:03:59.666898Z","iopub.status.idle":"2023-10-30T12:03:59.674105Z","shell.execute_reply.started":"2023-10-30T12:03:59.666869Z","shell.execute_reply":"2023-10-30T12:03:59.673152Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'summary'],\n    num_rows: 16539\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:03:59.677787Z","iopub.execute_input":"2023-10-30T12:03:59.678114Z","iopub.status.idle":"2023-10-30T12:03:59.695193Z","shell.execute_reply.started":"2023-10-30T12:03:59.678087Z","shell.execute_reply":"2023-10-30T12:03:59.694350Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:03:59.696256Z","iopub.execute_input":"2023-10-30T12:03:59.696601Z","iopub.status.idle":"2023-10-30T12:03:59.703465Z","shell.execute_reply.started":"2023-10-30T12:03:59.696558Z","shell.execute_reply":"2023-10-30T12:03:59.702444Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'text': \"The Schedule and the rules continued without repeal or amendment when the new section III (1) was substituted in 1936, and when this section made a reference to the rules in Schedule IV it could only be a reference to the rules in the Schedule IV which stood ' unaltered. If the phraseology employed in the Schedule was inappropriate to a class which fell within section 111(1), the, only effect would be that the tax could not be levied, because 976 of the defect in the law imposing the tax, but such a situation is not remedied by reference to the provision in the General Clauses Act on which the learned Judges have relied. If, therefore, the, tax was one not lawfully levied just prior to April 1, 1937 and was one brought in after the Government of India Act, 1935 came into force, and really only from April 1, 1942 assuming this to be lawful it is obvious that the validity of this tax could not be sustained as a continuation of a lawful pre existing levy under section 143 (2). In this view it is not necessary to consider the last of the points urged by learned Counsel and examine whether in case of an increase of rate, the entire tax would become a new tax and so unconstitutional or whether it is only the increase in the rate that would become unenforceable. Learned Counsel for the respondent Corporation submitted that the tax could not be deemed to be a tax on income, as was suggested by the appellant, but was really a tax on employment because it was in consideration of past services during employment that pension was payable. This argument was admittedly not urged before the learned Judges of the High Court and is obviously untenable. The taxes specified in item 60 are taxes on the carrying on of a profession, trade, etc. and would, therefore, apply only to a case of present employment. The mere fact that a person has previously been in a profession or carried on a trade, etc. cannot justify a tax under this Entry. The tax on the receipt of pension or on the income from investments which is referred to in the last part of section 1 1 1 ( 1 ) is in truth and substance a tax on income and in fact the argument before the High Court proceeded on this basis, so have the learned Judges. At the time the tax is levied the pensioner is in no employment but is only in receipt of income though it might be for past services, in an employment. He next submitted that Act X of 1936 which had been enacted prior to the Government of India Act, 1935 was continued as an existing law by section 292 of the Government of India Act and as there was nothing in the Government of India Act against its continuance it would have effect 977 even if the terms of section 143 (2) were not satisfied by the present levy. The learned Judges of the High Court accepted this submission. In our opinion, they were in order.\",\n 'summary': 'On the facts of this case it was held that if the statutory charge to profession tax imposed on pensioners by the Act of 1919, was lifted by the Act of 1936, and the tax again came into operation only on April 1, 1937, it would follow that there was no \"levy of the tax \\' immediately before the commencement of Part III of the Government of India Act.The Corporation of Madras demanded profession tax from him under section 111(i)(b) of the City Municipal Act, 1919 for the year 1958 59 on the ground of his residence being within Madras city and his drawing the pension to which he was entitled.It is not the intention of Parliament that State might levy a tax on income and call it \"profession\" tax.The High Court granted a certificate under Art.133(1)(c ) of the Constitution to the appellant to file on appeal to the Supreme Court.At the time the tax is levied the appellant pen sioner is inno employment but is only in receipt of income.15,000/ per annum as pension while residing in the city of Madras.'}"},"metadata":{}}]},{"cell_type":"markdown","source":"The next step is to load a T5 tokenizer to process `text` and `summary`:","metadata":{"id":"jzOwaH4CZlQp"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"id":"-bI5SYtmZlQq","execution":{"iopub.status.busy":"2023-10-30T12:03:59.704445Z","iopub.execute_input":"2023-10-30T12:03:59.704754Z","iopub.status.idle":"2023-10-30T12:04:00.835076Z","shell.execute_reply.started":"2023-10-30T12:03:59.704730Z","shell.execute_reply":"2023-10-30T12:04:00.833966Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The preprocessing function you want to create needs to:\n\n1. Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n2. Use the keyword `text_target` argument when tokenizing labels.\n3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter.","metadata":{"id":"AWFxp40HZlQq"}},{"cell_type":"code","source":"prefix = \"summarize: \"\n\n\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"text\"]]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n\n    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"id":"OT8EPhRaZlQq","execution":{"iopub.status.busy":"2023-10-30T12:04:00.836319Z","iopub.execute_input":"2023-10-30T12:04:00.837049Z","iopub.status.idle":"2023-10-30T12:04:00.843177Z","shell.execute_reply.started":"2023-10-30T12:04:00.837015Z","shell.execute_reply":"2023-10-30T12:04:00.842078Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"To apply the preprocessing function over the entire dataset, use 🤗 Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:","metadata":{"id":"orgkDuBRZlQq"}},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"id":"V3nOEoGVZlQr","execution":{"iopub.status.busy":"2023-10-30T12:04:00.844486Z","iopub.execute_input":"2023-10-30T12:04:00.844797Z","iopub.status.idle":"2023-10-30T12:04:24.997759Z","shell.execute_reply.started":"2023-10-30T12:04:00.844771Z","shell.execute_reply":"2023-10-30T12:04:24.996858Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ace532d9803446cfa380f67af5e9926c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68aa52263c9641a294356f3ac7cac149"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now create a batch of examples using [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.","metadata":{"id":"wS0Vpkr8ZlQr"}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)","metadata":{"id":"17l-1G4BZlQr","execution":{"iopub.status.busy":"2023-10-30T12:04:24.999197Z","iopub.execute_input":"2023-10-30T12:04:24.999530Z","iopub.status.idle":"2023-10-30T12:04:29.387994Z","shell.execute_reply.started":"2023-10-30T12:04:24.999501Z","shell.execute_reply":"2023-10-30T12:04:29.386990Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluate","metadata":{"id":"KW34cOECZlQr"}},{"cell_type":"markdown","source":"Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):","metadata":{"id":"avh76AkSZlQr"}},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load(\"rouge\")","metadata":{"id":"dGLFI0znZlQr","execution":{"iopub.status.busy":"2023-10-30T12:04:29.389074Z","iopub.execute_input":"2023-10-30T12:04:29.389688Z","iopub.status.idle":"2023-10-30T12:04:31.071567Z","shell.execute_reply.started":"2023-10-30T12:04:29.389658Z","shell.execute_reply":"2023-10-30T12:04:31.070472Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the ROUGE metric:","metadata":{"id":"F5ReEPWHZlQr"}},{"cell_type":"code","source":"import numpy as np\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"id":"657ymyU6ZlQs","execution":{"iopub.status.busy":"2023-10-30T12:04:31.073030Z","iopub.execute_input":"2023-10-30T12:04:31.073417Z","iopub.status.idle":"2023-10-30T12:04:31.081551Z","shell.execute_reply.started":"2023-10-30T12:04:31.073381Z","shell.execute_reply":"2023-10-30T12:04:31.080341Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.","metadata":{"id":"EXLqn4N6ZlQs"}},{"cell_type":"markdown","source":"## Train","metadata":{"id":"ns_Z9gJzZlQs"}},{"cell_type":"markdown","source":"<Tip>\n\nIf you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load T5 with [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):","metadata":{"id":"wwVCGUbUZlQs"}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)","metadata":{"id":"f7-I2TL_ZlQs","execution":{"iopub.status.busy":"2023-10-30T12:04:31.084775Z","iopub.execute_input":"2023-10-30T12:04:31.085072Z","iopub.status.idle":"2023-10-30T12:04:34.129350Z","shell.execute_reply.started":"2023-10-30T12:04:31.085046Z","shell.execute_reply":"2023-10-30T12:04:34.128545Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"At this point, only three steps remain:\n\n1. Define your training hyperparameters in [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the ROUGE metric and save the training checkpoint.\n2. Pass the training arguments to [Seq2SeqTrainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model.","metadata":{"id":"rPJmbHT8ZlQs"}},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:04:34.130607Z","iopub.execute_input":"2023-10-30T12:04:34.130907Z","iopub.status.idle":"2023-10-30T12:04:34.135457Z","shell.execute_reply.started":"2023-10-30T12:04:34.130881Z","shell.execute_reply":"2023-10-30T12:04:34.134264Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"t5-base-cls\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=10,\n    predict_with_generate=True,\n    fp16=True,\n    push_to_hub=False,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"id":"JYc7D_F5ZlQs","execution":{"iopub.status.busy":"2023-10-30T12:04:34.136872Z","iopub.execute_input":"2023-10-30T12:04:34.137202Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mastro2105\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231030_120440-m76ki4em</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/astro2105/huggingface/runs/m76ki4em' target=\"_blank\">costumed-ghoul-21</a></strong> to <a href='https://wandb.ai/astro2105/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/astro2105/huggingface' target=\"_blank\">https://wandb.ai/astro2105/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/astro2105/huggingface/runs/m76ki4em' target=\"_blank\">https://wandb.ai/astro2105/huggingface/runs/m76ki4em</a>"},"metadata":{}},{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='114' max='8270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 114/8270 02:56 < 3:34:22, 0.63 it/s, Epoch 0.14/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:","metadata":{"id":"5N5dBK6CZlQs"}},{"cell_type":"code","source":"# Define additional training arguments for the next phase of training\nadditional_training_args = Seq2SeqTrainingArguments(\n    output_dir=\"my_awesome_dataset_model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    save_total_limit=1\n    num_train_epochs=10,  # Train for an additional 10 epochs\n    predict_with_generate=True,\n    fp16=True,\n    push_to_hub=False,\n)\n\n# Create a new trainer for additional training\nadditional_trainer = Seq2SeqTrainer(\n    model=model,\n    args=additional_training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Continue training for additional epochs\nadditional_trainer.train()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\nimport os\n\n# Define the directory where you want to save the model\noutput_directory = \"model\"\n\n# Create the directory if it doesn't exist\nif not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n# Load a pre-trained model and tokenizer (replace with your own model and tokenizer)\nmodel_name = \"t5-basev1\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Save the model, tokenizer, and configuration to the specified directory\nmodel.save_pretrained(output_directory)\ntokenizer.save_pretrained(output_directory)\n\n","metadata":{"id":"BvjclM12ZlQs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n\n# Define the name for your model on Hugging Face Hub\nhub_model_name = \"astro21/t5-base-cls\"\n\n# Save the model and tokenizer to the Hugging Face Model Hub\nbart_model.push_to_hub(hub_model_name)\ntokenizer.push_to_hub(hub_model_name)\n\n# Once the above is done, you can also save the configuration for the model\nbart_model.config.push_to_hub(hub_model_name)\n\n# Commit your changes\nbart_model.push_to_hub(hub_model_name, commit_message=\"Initial commit\")\n\nprint(f\"Model and tokenizer are now available on the Hugging Face Model Hub with the name: {hub_model_name}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<Tip>\n\nFor a more in-depth example of how to finetune a model for summarization, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).\n\n</Tip>","metadata":{"id":"4qVqJH-VZlQs"}},{"cell_type":"code","source":"text = dataset[\"test\"][\"text\"]","metadata":{"id":"_Df7SqNhZlQt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for summarization with your model, and pass your text to it:","metadata":{"id":"nE-QAlVsZlQw"}},{"cell_type":"code","source":"# !zip -r file.zip /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'file.zip')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # Load the saved model and tokenizer for testing\n# model = AutoModelForSeq2SeqLM.from_pretrained(output_directory)\n# tokenizer = AutoTokenizer.from_pretrained(output_directory)\n\n\n\n# # Define the maximum chunk size (in tokens)\n# max_chunk_size = 1024  # Adjust as needed\n\n# # Split the text into manageable chunks\n# text_chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n\n# # Initialize an empty list to store individual summaries\n# individual_summaries = []\n\n# # Generate summaries for each chunk separately\n# for chunk in text_chunks:\n#     # Tokenize the chunk\n#     tokenized_input = tokenizer(\"summarize: \" + chunk, truncation=True, max_length=max_chunk_size)\n\n#     # Generate the summary for the chunk\n#     summary = model.generate(tokenized_input[\"input_ids\"], max_length=128, do_sample=False)\n\n#     # Decode the generated summary\n#     generated_summary = tokenizer.decode(summary[0], skip_special_tokens=True)\n    \n#     print(generated_summary)\n\n#     # Append the individual summary to the list\n#     individual_summaries.append(generated_summary)\n\n# # Concatenate individual summaries into a single summary for the entire document\n# full_document_summary = \" \".join(individual_summaries)\n\n# # Print or save the full document summary\n# print(full_document_summary)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}